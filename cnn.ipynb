{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Data Imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "import helper_functions as hf\n",
    "\n",
    "# Deep Learning Framework\n",
    "import tensorflow as tf\n",
    "\n",
    "# SSL\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"* {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paths\n",
    "\n",
    "train_data_path = './data/archive/train'\n",
    "test_data_path = './data/archive/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classes are: ['daisy', 'rose', 'tulip', 'dandelion', 'sunflower']\n"
     ]
    }
   ],
   "source": [
    "# Viewing the classes\n",
    "\n",
    "categories = os.listdir(train_data_path)\n",
    "print(f\"The classes are: {categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "\n",
    "SEED = 0\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.20\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2746 files belonging to 5 classes.\n",
      "Using 2197 files for training.\n"
     ]
    }
   ],
   "source": [
    "# Creating the training set\n",
    "\n",
    "training_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_data_path,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=categories\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2746 files belonging to 5 classes.\n",
      "Using 549 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Creating the validation set\n",
    "\n",
    "validation_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_data_path,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting some images\n",
    "\n",
    "# hf.plot_images(training_set, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates a model\n",
    "\n",
    "def get_baseline_model(): \n",
    "\n",
    "    ## Clearing backend\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    ## Input Layer\n",
    "    inputs = tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "    # Rescaling the images\n",
    "    x = tf.keras.layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "    ## First CNN\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        activation='relu',\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.MaxPool2D(\n",
    "        pool_size=(2, 2)\n",
    "    )(x)\n",
    "\n",
    "    ## Second CNN\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        activation='relu',\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.MaxPool2D(\n",
    "        pool_size=(2, 2)\n",
    "    )(x)\n",
    "\n",
    "    ## Third CNN\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        activation='relu',\n",
    "    )(x)\n",
    "\n",
    "    x = tf.keras.layers.MaxPool2D(\n",
    "        pool_size=(2, 2)\n",
    "    )(x)\n",
    "\n",
    "    ## Flatten layer\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    ## First Dense layer\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=64,\n",
    "        activation='relu'\n",
    "    )(x)\n",
    "\n",
    "    ## Output\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=len(categories),\n",
    "        activation='softmax'\n",
    "    )(x)\n",
    "\n",
    "    ## Creating Model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs\n",
    "    )\n",
    "\n",
    "    ## Compiling the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    ## Viewing the architecture\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_elapsed_time = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training the model\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# baseline_model = get_baseline_model()\n",
    "\n",
    "# history = baseline_model.fit(\n",
    "#   training_set,\n",
    "#   validation_data=validation_set,\n",
    "#   epochs=EPOCHS\n",
    "# )\n",
    "\n",
    "# elasped_time = time.time() - start_time\n",
    "# model_elapsed_time[\"baseline\"] = elasped_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Viewing the results of the training\n",
    "\n",
    "# hf.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf.plot_actual_prediction(baseline_model, categories, validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is not performing well. Accuracy is terrible (bias is high). And generalization is bad (variance is high). \n",
    "\n",
    "Since the model did not accurately predict the flowers, the model didn't learn key features that differentiates the flowers (categories).\n",
    "\n",
    "Solutions:\n",
    "<ul>\n",
    "    <li> Data Augmentation </li>\n",
    "         * Create new images by augmenting the images to expose the model to more images\n",
    "    <li> Transfer Learning</li>\n",
    "        * Use a working model that performs well for our task<br>\n",
    "    <li> Get more data</li>\n",
    "        * Find more data for the model\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\n",
    "      \"horizontal_and_vertical\", \n",
    "      input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[0], 3)\n",
    "  ),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "  tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "  tf.keras.layers.RandomCrop(\n",
    "    128, 128\n",
    "  ),\n",
    "  tf.keras.layers.Resizing(IMAGE_SIZE[0], IMAGE_SIZE[0])\n",
    "])\n",
    "\n",
    "def prepare(ds, shuffle=False, augment=False):\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "\n",
    "    # Use data augmentation only on the training set.\n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y), \n",
    "            num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "    # Use buffered prefetching on all datasets.\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n"
     ]
    }
   ],
   "source": [
    "train_ds = prepare(training_set, shuffle=True, augment=True)\n",
    "val_ds = prepare(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Running the model\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# augment_model = get_baseline_model()\n",
    "\n",
    "# augment_history = augment_model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data = val_ds,\n",
    "#     epochs=EPOCHS\n",
    "# )\n",
    "\n",
    "# elasped_time = time.time() - start_time\n",
    "# model_elapsed_time['data_augmentation'] = elasped_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Viewing the results of the training\n",
    "\n",
    "# hf.plot_history(augment_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_model.save('saved_model/augment_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation have similar accuracy and loss (low variance). However the accuracy is ~70% (high bias). If we would like, we can do more augmentation like random contrasting.\n",
    "\n",
    "Next we will try transfer learning. Use a model that has a great track record classifying and apply it to our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_transfer_model = tf.keras.applications.MobileNetV3Small(\n",
    "#     input_shape=IMAGE_SIZE+ (3,),\n",
    "#     include_top=False,\n",
    "#     weights='imagenet',\n",
    "# )\n",
    "\n",
    "# base_transfer_model.trainable = False\n",
    "\n",
    "# base_transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_batch, label_batch = next(iter(train_ds))\n",
    "# feature_batch = base_transfer_model(image_batch)\n",
    "# print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "# x = data_augmentation(inputs)\n",
    "# x = base_transfer_model(x, training=False)\n",
    "# x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "# outputs = tf.keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "# transfer_model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start_time = time.time()\n",
    "\n",
    "# transfer_model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # elasped_time = time.time() - start_time\n",
    "# # model_elapsed_time['transfer_model'] = elasped_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_history = transfer_model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data = val_ds,\n",
    "#     epochs=EPOCHS\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's take a look to see how many layers are in the base model\n",
    "# print(\"Number of layers in the base model: \", len(base_transfer_model.layers))\n",
    "\n",
    "# # Fine-tune from this layer onwards\n",
    "# fine_tune_at = 200\n",
    "\n",
    "# # Freeze all the layers before the `fine_tune_at` layer\n",
    "# for layer in base_transfer_model.layers[fine_tune_at:]:\n",
    "#     layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# transfer_fine = transfer_model.fit(\n",
    "#     train_ds,\n",
    "#     epochs=EPOCHS+10,\n",
    "#     initial_epoch=transfer_history.epoch[-1],\n",
    "#     validation_data=val_ds\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf.plot_history(transfer_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_model.save('saved_model/transfer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tenkeras.models import load_model\n",
    "# loaded_model = tf.keras.models.load_model(\"./saved_model/transfer_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# new_image_path = './data/FREEDOM.jpg'\n",
    "# image = Image.open(new_image_path).resize((224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = np.expand_dims(tf.keras.preprocessing.image.img_to_array(image), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = loaded_model.predict(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet_model = tf.keras.applications.resnet50.ResNet50(\n",
    "#     input_shape=IMAGE_SIZE+ (3,),\n",
    "#     include_top=False,\n",
    "#     weights='imagenet',\n",
    "# )\n",
    "\n",
    "# resnet_model.trainable = False\n",
    "\n",
    "# # Let's take a look to see how many layers are in the base model\n",
    "# print(\"Number of layers in the base model: \", len(resnet_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet_model.trainable = False\n",
    "\n",
    "# # Fine-tune from this layer onwards\n",
    "# fine_tune_at = 170\n",
    "\n",
    "# # Freeze all the layers before the `fine_tune_at` layer\n",
    "# for layer in resnet_model.layers[fine_tune_at:]:\n",
    "#     layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Distributed Datasets from the datasets\n",
    "# train_ds = mirrored_strategy.experimental_distribute_dataset(train_ds)\n",
    "# val_ds = mirrored_strategy.experimental_distribute_dataset(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "\n",
    "#     inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "#     x = resnet_model(inputs, training=False)\n",
    "#     x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(256)(x)\n",
    "#     x = tf.keras.layers.Dropout(0.5)(x)\n",
    "#     outputs = tf.keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "    \n",
    "#     return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "#     monitor='val_loss', \n",
    "#     factor=0.2,\n",
    "#     patience=2, \n",
    "#     min_lr=0.001\n",
    "# )\n",
    "\n",
    "# # es = tf.keras.callbacks.EarlyStopping(\n",
    "# #     monitor='val_loss',\n",
    "# #     min_delta=0,\n",
    "# #     patience=3,\n",
    "# #     verbose=0,\n",
    "# #     mode='auto',\n",
    "# #     baseline=None,\n",
    "# #     restore_best_weights=False,\n",
    "# # )\n",
    "\n",
    "# with mirrored_strategy.scope():\n",
    "    \n",
    "#     resnet_model = tf.keras.applications.resnet50.ResNet50(\n",
    "#         input_shape=IMAGE_SIZE+ (3,),\n",
    "#         include_top=False,\n",
    "#         weights='imagenet',\n",
    "#     )\n",
    "\n",
    "#     resnet_model.trainable = False\n",
    "    \n",
    "#     inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "#     x = resnet_model(inputs, training=False)\n",
    "#     x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(256)(x)\n",
    "#     x = tf.keras.layers.Dropout(0.5)(x)\n",
    "#     outputs = tf.keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "    \n",
    "#     transfer_resnet_model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "#     transfer_resnet_model.compile(\n",
    "#         optimizer='adam',\n",
    "#         loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "\n",
    "#     transfer_fine = transfer_resnet_model.fit(\n",
    "#         train_ds,\n",
    "#         epochs=20,\n",
    "#         # initial_epoch=transfer_history.epoch[-1],\n",
    "#         validation_data=val_ds,\n",
    "#         callbacks=[reduce_lr]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 13:31:43.661884: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "69/69 [==============================] - 90s 1s/step - loss: 1.6587 - accuracy: 0.6468 - val_loss: 0.7771 - val_accuracy: 0.8215 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "69/69 [==============================] - 86s 1s/step - loss: 0.9476 - accuracy: 0.7487 - val_loss: 0.5969 - val_accuracy: 0.8397 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "69/69 [==============================] - 87s 1s/step - loss: 0.7773 - accuracy: 0.7802 - val_loss: 0.7426 - val_accuracy: 0.8160 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "69/69 [==============================] - 88s 1s/step - loss: 0.7685 - accuracy: 0.7883 - val_loss: 0.7527 - val_accuracy: 0.8251 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "69/69 [==============================] - 86s 1s/step - loss: 0.6641 - accuracy: 0.8061 - val_loss: 0.5508 - val_accuracy: 0.8470 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "69/69 [==============================] - 85s 1s/step - loss: 0.6898 - accuracy: 0.8093 - val_loss: 0.6795 - val_accuracy: 0.8361 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "69/69 [==============================] - 86s 1s/step - loss: 0.6074 - accuracy: 0.8234 - val_loss: 0.7375 - val_accuracy: 0.8124 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "69/69 [==============================] - 87s 1s/step - loss: 0.5706 - accuracy: 0.8275 - val_loss: 0.6873 - val_accuracy: 0.8342 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "69/69 [==============================] - 88s 1s/step - loss: 0.4941 - accuracy: 0.8421 - val_loss: 0.6131 - val_accuracy: 0.8270 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "69/69 [==============================] - 86s 1s/step - loss: 0.5440 - accuracy: 0.8284 - val_loss: 0.7030 - val_accuracy: 0.8324 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "69/69 [==============================] - 85s 1s/step - loss: 0.4532 - accuracy: 0.8584 - val_loss: 0.6839 - val_accuracy: 0.8342 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "69/69 [==============================] - 86s 1s/step - loss: 0.4541 - accuracy: 0.8462 - val_loss: 0.5146 - val_accuracy: 0.8506 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "69/69 [==============================] - 87s 1s/step - loss: 0.4525 - accuracy: 0.8443 - val_loss: 0.5430 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "69/69 [==============================] - 88s 1s/step - loss: 0.4710 - accuracy: 0.8366 - val_loss: 0.5568 - val_accuracy: 0.8415 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "69/69 [==============================] - 87s 1s/step - loss: 0.3982 - accuracy: 0.8584 - val_loss: 0.4455 - val_accuracy: 0.8707 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "69/69 [==============================] - 86s 1s/step - loss: 0.3967 - accuracy: 0.8639 - val_loss: 0.5150 - val_accuracy: 0.8379 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "22/69 [========>.....................] - ETA: 47s - loss: 0.3476 - accuracy: 0.8580"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb Cell 48\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m transfer_resnet_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs, outputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m transfer_resnet_model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mCategoricalCrossentropy(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m transfer_fine \u001b[39m=\u001b[39m transfer_resnet_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     train_ds,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m# initial_epoch=transfer_history.epoch[-1],\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_ds,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[reduce_lr]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/cnn.ipynb#X65sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2,\n",
    "    patience=2, \n",
    "    min_lr=0.001\n",
    ")\n",
    "\n",
    "resnet_model = tf.keras.applications.resnet50.ResNet50(\n",
    "    input_shape=IMAGE_SIZE+ (3,),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    ")\n",
    "\n",
    "resnet_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = resnet_model(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(256)(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "\n",
    "transfer_resnet_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "transfer_resnet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "transfer_fine = transfer_resnet_model.fit(\n",
    "    train_ds,\n",
    "    epochs=20,\n",
    "    # initial_epoch=transfer_history.epoch[-1],\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
