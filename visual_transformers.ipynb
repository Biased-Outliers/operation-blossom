{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# System Imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Data Imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import cv2\n",
    "\n",
    "import helper_functions as hf\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import ViTFeatureExtractor, TFViTForImageClassification\n",
    "import datasets\n",
    "\n",
    "tf.keras.backend.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paths\n",
    "\n",
    "train_data_path = './data/archive/train'\n",
    "test_data_path = './data/archive/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categories are: ['daisy', 'rose', 'tulip', 'dandelion', 'sunflower']\n"
     ]
    }
   ],
   "source": [
    "# Viewing categories\n",
    "\n",
    "categories = os.listdir(train_data_path)\n",
    "print(f\"The categories are: {categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "\n",
    "SEED = 0\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.20\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_folder_dataset(root_path):\n",
    "  \"\"\"creates `Dataset` from image folder structure\"\"\"\n",
    "\n",
    "  # get class names by folders names\n",
    "  _CLASS_NAMES= os.listdir(root_path)\n",
    "  # defines `datasets` features`\n",
    "  features=datasets.Features({\n",
    "                      \"img\": datasets.Image(),\n",
    "                      \"label\": datasets.features.ClassLabel(names=_CLASS_NAMES),\n",
    "                  })\n",
    "  # temp list holding datapoints for creation\n",
    "  img_data_files=[]\n",
    "  label_data_files=[]\n",
    "  # load images into list for creation\n",
    "  for img_class in os.listdir(root_path):\n",
    "    for img in os.listdir(os.path.join(root_path,img_class)):\n",
    "      path_=os.path.join(root_path,img_class,img)\n",
    "      img_data_files.append(path_)\n",
    "      label_data_files.append(img_class)\n",
    "  # create dataset\n",
    "  ds = datasets.Dataset.from_dict({\"img\":img_data_files,\"label\":label_data_files},features=features)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_image_folder_dataset(\"./data/archive/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['img', 'label'],\n",
       "    num_rows: 2746\n",
       "})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daisy', 'rose', 'tulip', 'dandelion', 'sunflower']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_class_labels = df.features[\"label\"].names\n",
    "img_class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2746 files belonging to 5 classes.\n",
      "Using 2197 files for training.\n"
     ]
    }
   ],
   "source": [
    "# Creating the training set\n",
    "\n",
    "training_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_data_path,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=categories\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2746 files belonging to 5 classes.\n",
      "Using 549 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Creating the validation set\n",
    "\n",
    "validation_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_data_path,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the layers of TFViTForImageClassification were initialized from the model checkpoint at google/vit-base-patch16-224.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'google/vit-base-patch16-224'\n",
    "model = TFViTForImageClassification.from_pretrained(model_path)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_path)\n",
    "# feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing channel axis to the first dimension\n",
    "\n",
    "train = training_set.map(lambda x, y: (tf.experimental.numpy.moveaxis(x, -1, 1), y)) \n",
    "valid = validation_set.map(lambda x, y: (tf.experimental.numpy.moveaxis(x, -1, 1), y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_images(examples):\n",
    "\n",
    "#     print(examples.shape)\n",
    "#     images = examples['img']\n",
    "#     images = [np.array(image, dtype=np.uint8) for image in images]\n",
    "#     # images = [np.moveaxis(image, source=-1, destination=0) for image in images]\n",
    "#     inputs = feature_extractor(images=images)\n",
    "#     examples['pixel_values'] = inputs['pixel_values']\n",
    "\n",
    "#     return examples.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(image, label):\n",
    "    inputs = feature_extractor(image.numpy(), return_tensors='tf')\n",
    "    print(inputs)\n",
    "    inputs['labels'] = label\n",
    "    return inputs\n",
    "\n",
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x for x in example_batch[0]], return_tensors='tf')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch[1]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  3 224 224], shape=(3,), dtype=int32) tf.Tensor([5], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for image, label in train.take(1):\n",
    "    # print(tf.shape(feature_extractor(image[0].numpy(), return_tensors='tf')['pixel_values']))\n",
    "    # print(tf.shape(image[0]), tf.shape(label[0]))\n",
    "    # print(feature_extractor(image[0].numpy()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DatasetV2.map() got an unexpected keyword argument 'batched'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mmap(process_example, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m valid \u001b[39m=\u001b[39m valid\u001b[39m.\u001b[39mmap(process_example, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: DatasetV2.map() got an unexpected keyword argument 'batched'"
     ]
    }
   ],
   "source": [
    "train = train.map(process_example) \n",
    "valid = valid.map(process_example) \n",
    "\n",
    "# results = []\n",
    "# for image,label in temp_set.take(1):\n",
    "#     results.append(process_example([image, label]))\n",
    "#     # print(tf.shape(data[1]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "pixel_values = tf.keras.layers.Input(shape=(3,224,224), name='pixel_values', dtype='float32')\n",
    "\n",
    "# model layer\n",
    "vit = model.vit(pixel_values)[0]\n",
    "classifier = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(vit[:, 0, :])\n",
    "\n",
    "# model\n",
    "keras_model = tf.keras.Model(inputs=pixel_values, outputs=classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 3, 224, 224), found shape=(None, 224, 224, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m## Compiling the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m keras_model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mCategoricalCrossentropy(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m vit_history \u001b[39m=\u001b[39m keras_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   training_set,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   validation_data\u001b[39m=\u001b[39;49mvalidation_set,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   epochs\u001b[39m=\u001b[39;49mEPOCHS\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tqrahman/Desktop/Biased_Outliers/blossom/visual_transformers.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m elasped_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/3h/n7g9d8x521gg4nfvf31y3_840000gn/T/__autograph_generated_file_wsvaxx_.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 3, 224, 224), found shape=(None, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "## Compiling the model\n",
    "keras_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "vit_history = keras_model.fit(\n",
    "  training_set,\n",
    "  validation_data=validation_set,\n",
    "  epochs=EPOCHS\n",
    ")\n",
    "\n",
    "elasped_time = time.time() - start_time\n",
    "# model_elapsed_time[\"vit\"] = elasped_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
